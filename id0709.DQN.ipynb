{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN 算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主要步骤\n",
    "* Replay Buffer 经验回放\n",
    "* Q-Network模型，使用神经网络来逼近Q函数\n",
    "* 目标Q-Network模型\n",
    "* 训练模型，利用经验回放和目标网络更新Q-Network的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的库\n",
    "pip install gym numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 导入相关库并定义经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random \n",
    "from collections import deque # 双端队列\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replay buffer\n",
    "'''\n",
    "三个函数功能：初始化，存储样本，取出样本，返回样本数量\n",
    "\n",
    "'''\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 定义QNetwork网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "    self.fc1 = nn.Linear(state_dim, 128)\n",
    "    self.fc2 = nn.Linear(128,128)\n",
    "    self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = torch.relu(self,fc1(x))\n",
    "    x = torch.relu(self, fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 定义DQN智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    '''\n",
    "    函数说明：初始化，包括网络，目标网络，优化器，经验回放池，超参数等\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, device):\n",
    "        self.device = device\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict()) # load_state_dict()用于加载模型的参数\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.eposilon = 1.0\n",
    "        self.eposilon_decay = 0.995\n",
    "        self.eposilon_min = 0.01\n",
    "        self.update_target_every = 1000\n",
    "        \n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.eposilon:\n",
    "            return random.randrange(2)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsequeeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return self.q_network(state).argmax(1).item()\n",
    "            \n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self, batch_size)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = torch.LongTensor(action).unqueeze(1).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).unqueeze(1).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        done = torch.FloatTensor(done).unqueeze(1).to(self.device)\n",
    "\n",
    "        q_values = self.q_network(state).gather(1, action)\n",
    "        next_q_values = self.target_network(next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = reward + self.gamma * next_q_values * (1 - done)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.eposilon > self.eposilon_min:\n",
    "            self.eposilon *= self.eposilon_decay\n",
    "\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter % self.update_target_every == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 训练DQN智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, env, num_episodes):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Reward: {total_reward}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, device)\n",
    "num_episodes = 200\n",
    "episode_rewards = train_dqn(agent, env, num_episodes)\n",
    "\n",
    "# 绘制出曲线\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlable('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('DQN on CartPole-v1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
